{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10c6f1-3171-48db-9e35-9de13dd11eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import requests\n",
    "import itertools\n",
    "import optuna\n",
    "import csv\n",
    "from scipy.stats import mannwhitneyu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from rdkit import RDLogger\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw, rdMolTransforms\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "IPythonConsole.drawOptions.addAtomIndices = True\n",
    "IPythonConsole.molSize = 300,300\n",
    " \n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import SAGEConv, GCNConv, GATConv, GINConv\n",
    "from torch_geometric.data import Data, Dataset, InMemoryDataset#, DataLoader depreciated, use below\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28316b93-eb27-477b-bfeb-a2166c7dd77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self, conv_type, hidden_channels, num_features, batch_norm=False, weight_init=True, dropout_rate=0.0):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = conv_type(num_features, hidden_channels*2)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels*2) if batch_norm else None #BatchNorm1D -> GraphNorm\n",
    "        \n",
    "        self.conv2 = conv_type(hidden_channels*2, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels) if batch_norm else None\n",
    "        \n",
    "        self.conv3 = conv_type(hidden_channels, hidden_channels//2)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "        self.lin = torch.nn.Linear(hidden_channels//2, 1)\n",
    "        if weight_init:\n",
    "            self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Apply Xavier initialisation to the weights of a given module.\"\"\"\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, torch_geometric.nn.MessagePassing):\n",
    "            # Initialising weights of the MessagePassing (convolution) layers\n",
    "            for param in module.parameters():\n",
    "                if param.dim() > 1:  # Only initialise weights, not biases\n",
    "                    torch.nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        if self.bn1:\n",
    "            x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        #if self.dropout:\n",
    "        #    x = self.dropout(x)\n",
    "            \n",
    "        x = self.conv2(x, edge_index)\n",
    "        if self.bn2:\n",
    "            x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        #if self.dropout:\n",
    "        #    x = self.dropout(x)\n",
    "            \n",
    "        x = self.conv3(x, edge_index)\n",
    "        #if self.bn3:\n",
    "        #    x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = torch_geometric.nn.global_mean_pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "class GINModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_features, batch_norm=False, weight_init=True, dropout_rate=0.0):\n",
    "        super(GINModel, self).__init__()\n",
    "        # Define MLP for GINConv\n",
    "        def mlp(input_dim, output_dim):\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_dim, output_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(output_dim, output_dim)\n",
    "            )\n",
    "\n",
    "        self.conv1 = torch_geometric.nn.GINConv(mlp(num_features, hidden_channels*2))\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels*2) if batch_norm else None\n",
    "        \n",
    "        self.conv2 = torch_geometric.nn.GINConv(mlp(hidden_channels*2, hidden_channels))\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels) if batch_norm else None\n",
    "        \n",
    "        self.conv3 = torch_geometric.nn.GINConv(mlp(hidden_channels, hidden_channels//2))\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "        self.lin = torch.nn.Linear(hidden_channels//2, 1)\n",
    "\n",
    "        # Apply Xavier initialisation\n",
    "        if weight_init:\n",
    "            self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Apply Xavier initialisation to the weights of a given module.\"\"\"\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, GINConv):\n",
    "            # Initialising weights of the GINConv's MLPs\n",
    "            for layer in module.nn:\n",
    "                if isinstance(layer, torch.nn.Linear):\n",
    "                    torch.nn.init.xavier_uniform_(layer.weight)\n",
    "                    if layer.bias is not None:\n",
    "                        torch.nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        if self.bn1:\n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(x) # Only if batch normalisation is performed\n",
    "        #if self.dropout:\n",
    "        #    x = self.dropout(x)\n",
    "            \n",
    "        x = self.conv2(x, edge_index)\n",
    "        if self.bn2:\n",
    "            x = self.bn2(x)\n",
    "            x = F.relu(x)\n",
    "        #if self.dropout:\n",
    "        #    x = self.dropout(x)\n",
    "            \n",
    "        x = self.conv3(x, edge_index)\n",
    "        #if self.bn3:\n",
    "        #    x = self.bn3(x)\n",
    "        #    x = F.relu(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = torch_geometric.nn.global_mean_pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec286ca-fa63-46d1-89bc-97b972f72bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimiser, criterion, device, threshold = 0.5): # added\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y.view(-1, 1).float())\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = (out > threshold).float() # added\n",
    "        correct += pred.eq(data.y.view(-1, 1)).sum().item()\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion, device, threshold = 0.5): # added\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y.view(-1, 1).float())\n",
    "            total_loss += loss.item()\n",
    "            pred = (out > threshold).float() # added\n",
    "            correct += pred.eq(data.y.view(-1, 1)).sum().item()\n",
    "            preds.append(out)\n",
    "            labels.append(data.y.view(-1, 1))\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    return total_loss / len(loader), correct / len(loader.dataset), roc_auc_score(labels.cpu(), preds.cpu()), f1_score(labels.cpu(), (preds > threshold).float().cpu()), matthews_corrcoef(labels.cpu(), (preds > threshold).float().cpu()) # added # modified for MCC\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_score, model):\n",
    "        score = -val_score\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_score, model)\n",
    "        elif score < self.best_score - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_score, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_score, model):\n",
    "        self.best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3440a-d7e7-4e14-a1ae-d3828bed69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_indices(pytorch_custom_dataset, seed=42):\n",
    "    num_samples = len(pytorch_custom_dataset)\n",
    "    indices = np.arange(num_samples)\n",
    "    targets = [data.y.item() for data in pytorch_custom_dataset]\n",
    "\n",
    "    skf_outer = StratifiedKFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "    outer_folds = []\n",
    "\n",
    "    for train_indices, test_indices in skf_outer.split(indices, targets):\n",
    "        train_targets = np.array(targets)[train_indices]\n",
    "        skf_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        inner_folds = []\n",
    "        for inner_train_idx, val_idx in skf_inner.split(np.arange(len(train_indices)), train_targets):\n",
    "            inner_train_indices = train_indices[inner_train_idx]\n",
    "            val_indices = train_indices[val_idx]\n",
    "            inner_folds.append((inner_train_indices, val_indices))\n",
    "        outer_folds.append((train_indices, test_indices, inner_folds)) \n",
    "    return outer_folds\n",
    "\n",
    "def objective(hyperparameters, model_class, conv_type, pytorch_custom_dataset, device, results_dir, model_name, outer_fold, inner_fold, fold_indices, use_early_stopping):\n",
    "    # Extract hyperparameters\n",
    "    learning_rate = hyperparameters['learning_rate']\n",
    "    hidden_channels = hyperparameters['hidden_channels']\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    dropout_rate = hyperparameters['dropout_rate']\n",
    "    num_epochs = 100  # Fixed number of epochs\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(1)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1)\n",
    "\n",
    "    # Get the indices for the current fold\n",
    "    train_indices, test_indices, inner_folds = fold_indices[outer_fold]\n",
    "    inner_train_indices, val_indices = inner_folds[inner_fold]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = Subset(pytorch_custom_dataset, inner_train_indices)\n",
    "    val_dataset = Subset(pytorch_custom_dataset, val_indices)\n",
    "    test_dataset = Subset(pytorch_custom_dataset, test_indices)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    num_features = pytorch_custom_dataset[0].num_features  # Assuming dataset has num_features attribute\n",
    "\n",
    "    # Initialize the model\n",
    "    if model_class == GINModel:\n",
    "        model = model_class(\n",
    "            hidden_channels=hidden_channels,\n",
    "            num_features=num_features,\n",
    "            batch_norm=False,\n",
    "            weight_init=True,\n",
    "            dropout_rate=dropout_rate\n",
    "        ).to(device)\n",
    "    else:\n",
    "        model = model_class(\n",
    "            conv_type=conv_type,\n",
    "            hidden_channels=hidden_channels,\n",
    "            num_features=num_features,\n",
    "            batch_norm=False,\n",
    "            weight_init=True,\n",
    "            dropout_rate=dropout_rate\n",
    "        ).to(device)\n",
    "\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Training loop with early stop function\n",
    "    early_stopping = EarlyStopping(patience = 10, delta=0)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_acc = train(model, train_loader, optimiser, criterion, device)\n",
    "        val_loss, val_acc, val_auc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        if use_early_stopping:\n",
    "            early_stopping(val_auc, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "    best_model = early_stopping.best_model if use_early_stopping else model\n",
    "    val_loss, val_acc, val_auc, _, _ = evaluate(best_model, val_loader, criterion, device)\n",
    "    \n",
    "    return val_auc\n",
    "\n",
    "def grid_search(model_class, conv_type, pytorch_custom_dataset, results_dir, model_name, use_early_stopping):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Get the fold indices once\n",
    "    fold_indices = get_fold_indices(pytorch_custom_dataset, seed=42)\n",
    "\n",
    "    # Define hyperparameter grid\n",
    "    batch_size_list = [32, 64, 128]\n",
    "    hidden_channels_list = [64, 128, 256]\n",
    "    learning_rate_list = [0.001, 0.0001, 0.00001]\n",
    "    dropout_rate_list = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "    hyperparameter_grid = list(itertools.product(\n",
    "        batch_size_list,\n",
    "        hidden_channels_list,\n",
    "        learning_rate_list,\n",
    "        dropout_rate_list\n",
    "    ))\n",
    "\n",
    "    # List to store results\n",
    "    results = []\n",
    "\n",
    "    for outer_fold in range(4):\n",
    "        print(f\"Outer Fold {outer_fold + 1}/4\")\n",
    "\n",
    "        for inner_fold in range(5):\n",
    "            print(f\"  Inner Fold {inner_fold + 1}/5\")\n",
    "\n",
    "            for batch_size, hidden_channels, learning_rate, dropout_rate in hyperparameter_grid:\n",
    "                # Create a dictionary of hyperparameters\n",
    "                hyperparameters_dict = {\n",
    "                    'batch_size': batch_size,\n",
    "                    'hidden_channels': hidden_channels,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'dropout_rate': dropout_rate\n",
    "                }\n",
    "\n",
    "                # Call the objective function\n",
    "                val_auc = objective(\n",
    "                    hyperparameters=hyperparameters_dict,\n",
    "                    model_class=model_class,\n",
    "                    conv_type=conv_type,\n",
    "                    pytorch_custom_dataset=pytorch_custom_dataset,\n",
    "                    device=device,\n",
    "                    results_dir=results_dir,\n",
    "                    model_name=model_name,\n",
    "                    outer_fold=outer_fold,\n",
    "                    inner_fold=inner_fold,\n",
    "                    fold_indices=fold_indices,\n",
    "                    use_early_stopping = use_early_stopping\n",
    "                )\n",
    "                print(f\"Testing hyperparameters: bs={batch_size}, hc={hidden_channels}, lr={learning_rate}, dr={dropout_rate} / val_auc: {val_auc}\")\n",
    "                \n",
    "                # Store the results\n",
    "                result = {\n",
    "                    'model_name': model_name,\n",
    "                    'outer_fold': outer_fold,\n",
    "                    'inner_fold': inner_fold,\n",
    "                    'batch_size': batch_size,\n",
    "                    'hidden_channels': hidden_channels,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'val_auc': val_auc\n",
    "                }\n",
    "\n",
    "                results.append(result)\n",
    "\n",
    "    # Return all results\n",
    "    return results\n",
    "\n",
    "def run_grid_search_for_all_models(models, pytorch_custom_dataset, results_dir, use_early_stopping):\n",
    "    all_results = []\n",
    "\n",
    "    for name, model_class in models.items():\n",
    "        print(f\"Running Grid Search for {name} model...\")\n",
    "\n",
    "        # Define the convolution type (conv_type) based on the model name\n",
    "        if \"GCN\" in name:\n",
    "            conv_type = GCNConv\n",
    "        elif \"GAT\" in name:\n",
    "            conv_type = GATConv\n",
    "        elif \"GraphSAGE\" in name:\n",
    "            conv_type = SAGEConv\n",
    "        else:\n",
    "            conv_type = None  # For GINModel, no conv_type is needed\n",
    "\n",
    "        # Run grid search for this model\n",
    "        results = grid_search(\n",
    "            model_class=model_class,\n",
    "            conv_type=conv_type,\n",
    "            pytorch_custom_dataset=pytorch_custom_dataset,\n",
    "            results_dir=results_dir,\n",
    "            model_name=name,\n",
    "            use_early_stopping = use_early_stopping\n",
    "        )\n",
    "\n",
    "        # Add results to all_results\n",
    "        all_results.extend(results)\n",
    "\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc215c6-237d-46a4-83a6-fa0d535267a2",
   "metadata": {},
   "source": [
    "#### Drug-induced liver injury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f096e3-d655-48d4-8eda-5d9a8ee26488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated from Final_feature_enginnering.ipynb\n",
    "df_dili_filtered_cleaned = pd.read_csv('chem_data/DILI/DILIst_standardised_cleaned.csv', index_col = 0)\n",
    "\n",
    "dlst_min = MoleculeDataset_minimal(dataframe = df_dili_filtered_cleaned,\n",
    "                              smiles_col = 'smiles',\n",
    "                             label_col = 'label',\n",
    "                             root = 'custom_data/minimal_feature/dlst_min'\n",
    "                            )\n",
    "dlst_cus_2_std = MoleculeDataset_updated(dataframe = df_dili_filtered_cleaned,\n",
    "                              smiles_col = 'smiles_std',\n",
    "                             label_col = 'label',\n",
    "                             root = 'custom_data/DILI/cus2_std/'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e1ce4-25e6-415e-82ee-93174d8f8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlst_cus_2_std\n",
    "models = {\n",
    "    'GCN_Optimised': GNNModel,\n",
    "    'GAT_Optimised': GNNModel,\n",
    "    'GraphSAGE_Optimised': GNNModel,\n",
    "    'GIN_Optimised': GINModel\n",
    "}\n",
    "\n",
    "results_dir = './results/grid_search/dlst'\n",
    "\n",
    "# Run Grid Search\n",
    "all_results = run_grid_search_for_all_models(\n",
    "    models,\n",
    "    dlst_cus_2_std,\n",
    "    results_dir,\n",
    "    use_early_stopping = True\n",
    ")\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(os.path.join(results_dir, 'grid_search_results.csv'), index=False)\n",
    "\n",
    "with open(os.path.join(results_dir, 'grid_search_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e9d63-35a2-4466-b58d-8872e6191b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlst_min\n",
    "models = {\n",
    "    'GCN_Optimised': GNNModel,\n",
    "    'GAT_Optimised': GNNModel,\n",
    "    'GraphSAGE_Optimised': GNNModel,\n",
    "    'GIN_Optimised': GINModel\n",
    "}\n",
    "\n",
    "results_dir = './results/grid_search/dlst_min'\n",
    "\n",
    "# Run Grid Search\n",
    "all_results = run_grid_search_for_all_models(\n",
    "    models,\n",
    "    dlst_min,\n",
    "    results_dir,\n",
    "    use_early_stopping = True\n",
    ")\n",
    "\n",
    "# Create DataFrame \n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save DataFrame\n",
    "results_df.to_csv(os.path.join(results_dir, 'grid_search_results.csv'), index=False)\n",
    "\n",
    "# Optionally, save the DataFrame as a pickle file\n",
    "with open(os.path.join(results_dir, 'grid_search_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d5e42-201f-4b9c-8342-34527f6f9daf",
   "metadata": {},
   "source": [
    "#### Blood-Brain Barrier Permeability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398904f5-2ee6-4bee-96ed-aeeac615c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bbbp_filtered_cleaned = pd.read_csv('chem_data/bbbp_standardised_cleaned.csv', index_col = 0)\n",
    "\n",
    "bbbp_min = MoleculeDataset_minimal(dataframe = df_bbbp_filtered_cleaned,\n",
    "                              smiles_col = 'smiles',\n",
    "                             label_col = 'label',\n",
    "                             root = 'custom_data/minimal_feature/bbbp_min'\n",
    "                            )\n",
    "bbbp_cus_2_std = MoleculeDataset_updated(dataframe = df_bbbp_filtered_cleaned,\n",
    "                              smiles_col = 'smiles_std',\n",
    "                             label_col = 'label',\n",
    "                             root = 'custom_data/bbbp/cus2_std/'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cde34d-b760-4b69-a964-a75e845335c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbbp_cus_2_std\n",
    "models = {\n",
    "    'GCN_Optimised': GNNModel,\n",
    "    'GAT_Optimised': GNNModel,\n",
    "    'GraphSAGE_Optimised': GNNModel,\n",
    "    'GIN_Optimised': GINModel\n",
    "}\n",
    "\n",
    "results_dir = './results/grid_search/bbbp'\n",
    "\n",
    "all_results = run_grid_search_for_all_models(\n",
    "    models,\n",
    "    bbbp_cus_2_std,\n",
    "    results_dir,\n",
    "    use_early_stopping = True\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(os.path.join(results_dir, 'grid_search_results.csv'), index=False)\n",
    "\n",
    "with open(os.path.join(results_dir, 'grid_search_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc3407-f187-424c-8fe9-9232072df248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbbp_min\n",
    "models = {\n",
    "    'GCN_Optimised': GNNModel,\n",
    "    'GAT_Optimised': GNNModel,\n",
    "    'GraphSAGE_Optimised': GNNModel,\n",
    "    'GIN_Optimised': GINModel\n",
    "}\n",
    "\n",
    "results_dir = './results/grid_search/bbbp_min'\n",
    "\n",
    "all_results = run_grid_search_for_all_models(\n",
    "    models,\n",
    "    bbbp_min,\n",
    "    results_dir,\n",
    "    use_early_stopping = True\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(os.path.join(results_dir, 'grid_search_results.csv'), index=False)\n",
    "\n",
    "with open(os.path.join(results_dir, 'grid_search_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec345438-5aa4-4f6e-92c2-0cca8d19ba77",
   "metadata": {},
   "source": [
    "#### BACE Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3b8e4-e3ec-400b-a9c7-8b32fd6bb966",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bace_filtered_cleaned = pd.read_csv('chem_data/bace_standardised_cleaned.csv', index_col = 0)\n",
    "\n",
    "bace_min = MoleculeDataset_minimal(dataframe = df_bace_filtered_cleaned,\n",
    "                              smiles_col = 'smiles',\n",
    "                             label_col = 'label',\n",
    "                             root = 'custom_data/minimal_feature/bace_min'\n",
    "                            )\n",
    "bace_cus_2_std = MoleculeDataset_updated(dataframe = df_bace_filtered_cleaned,\n",
    "                              smiles_col = 'smiles_std',\n",
    "                             label_col = 'label',\n",
    "                             root = 'custom_data/bace/cus2_std/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643137d8-0c3c-4b52-bd66-911152630f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bace_cus_2_std\n",
    "models = {\n",
    "    'GCN_Optimised': GNNModel,\n",
    "    'GAT_Optimised': GNNModel,\n",
    "    'GraphSAGE_Optimised': GNNModel,\n",
    "    'GIN_Optimised': GINModel\n",
    "}\n",
    "\n",
    "results_dir = './results/grid_search/bace'\n",
    "\n",
    "all_results = run_grid_search_for_all_models(\n",
    "    models,\n",
    "    bace_cus_2_std,\n",
    "    results_dir,\n",
    "    use_early_stopping = True\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(os.path.join(results_dir, 'grid_search_results.csv'), index=False)\n",
    "\n",
    "with open(os.path.join(results_dir, 'grid_search_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b378a-5b2e-4d3a-9770-1b6a75612556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bace_min\n",
    "models = {\n",
    "    'GCN_Optimised': GNNModel,\n",
    "    'GAT_Optimised': GNNModel,\n",
    "    'GraphSAGE_Optimised': GNNModel,\n",
    "    'GIN_Optimised': GINModel\n",
    "}\n",
    "\n",
    "results_dir = './results/grid_search/bace_min'\n",
    "\n",
    "all_results = run_grid_search_for_all_models(\n",
    "    models,\n",
    "    bace_min,\n",
    "    results_dir,\n",
    "    use_early_stopping = True\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(os.path.join(results_dir, 'grid_search_results.csv'), index=False)\n",
    "\n",
    "with open(os.path.join(results_dir, 'grid_search_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a5a6c-76ac-450f-b741-aaeb53d4f9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
