{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb10c6f1-3171-48db-9e35-9de13dd11eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import requests\n",
    "import itertools\n",
    "import optuna\n",
    "import csv\n",
    "from scipy.stats import mannwhitneyu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from rdkit import RDLogger\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw, rdMolTransforms\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "IPythonConsole.drawOptions.addAtomIndices = True\n",
    "IPythonConsole.molSize = 300,300\n",
    " \n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import SAGEConv, GCNConv, GATConv, GINConv\n",
    "from torch_geometric.data import Data, Dataset, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e47fd5-611c-4643-b4da-8920ec051195",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Customised preprocessing (Mols-To-Graph function)\n",
    "\n",
    "class Graph_basic(InMemoryDataset):\n",
    "    def __init__(self, dataframe, root, smiles_col='smiles', label_col='label', test=False, transform=None, pre_transform=None):\n",
    "        self.test = test\n",
    "        self.dataframe = dataframe\n",
    "        self.smiles_col = smiles_col\n",
    "        self.label_col = label_col\n",
    "        self._data = None\n",
    "        self.error_indices = []\n",
    "        super(Graph_basic, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return 'dataframe'\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data_test.pt' if self.test else 'data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        for index, mol in tqdm(self.dataframe.iterrows(), total=self.dataframe.shape[0]):\n",
    "            try:\n",
    "                mol_obj = Chem.MolFromSmiles(mol[self.smiles_col])\n",
    "                node_feats = self._get_node_features(mol_obj)\n",
    "                edge_feats = self._get_edge_features(mol_obj)\n",
    "                edge_index = self._get_adjacency_info(mol_obj)\n",
    "                label = self._get_labels(mol[self.label_col])\n",
    "\n",
    "                data = Data(x=node_feats, \n",
    "                            edge_index=edge_index,\n",
    "                            edge_attr=edge_feats,\n",
    "                            y=label,\n",
    "                            smiles=mol[self.smiles_col])\n",
    "                data_list.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing molecule at index {index}: {e}\")\n",
    "                self.error_indices.append(index)\n",
    "\n",
    "        if self.test:\n",
    "            torch.save(data_list, os.path.join(self.processed_dir, 'data_test.pt'))\n",
    "        else:\n",
    "            torch.save(data_list, os.path.join(self.processed_dir, 'data.pt'))\n",
    "\n",
    "    def _get_node_features(self, mol):\n",
    "        \"\"\" \n",
    "        Return a matrix / 2D array of the shape [Number of Nodes, Node Feature size]\n",
    "        with atomic number as the only node feature.\n",
    "        \"\"\"\n",
    "        all_node_feats = []\n",
    "\n",
    "        for atom in mol.GetAtoms():\n",
    "            node_feats = []    \n",
    "            node_feats.append(atom.GetAtomicNum())\n",
    "            all_node_feats.append(node_feats)\n",
    "\n",
    "        all_node_feats = np.asarray(all_node_feats)\n",
    "        return torch.tensor(all_node_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_edge_features(self, mol):\n",
    "        \"\"\" \n",
    "        Return a matrix / 2D array of the shape [Number of edges, Edge Feature size]\n",
    "        with bond type as the only edge feature.\n",
    "        \"\"\"\n",
    "        all_edge_feats = []\n",
    "        \n",
    "        for bond in mol.GetBonds():\n",
    "            edge_feats = []\n",
    "            edge_feats.append(bond.GetBondTypeAsDouble())\n",
    "            all_edge_feats += [edge_feats, edge_feats]\n",
    "\n",
    "        all_edge_feats = np.asarray(all_edge_feats)\n",
    "        return torch.tensor(all_edge_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_adjacency_info(self, mol):\n",
    "        edge_indices = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            edge_indices += [[i, j], [j, i]]\n",
    "\n",
    "        edge_indices = torch.tensor(edge_indices)\n",
    "        edge_indices = edge_indices.t().to(torch.long).view(2, -1)\n",
    "        return edge_indices\n",
    "\n",
    "    def _get_labels(self, label):\n",
    "        label = np.asarray([[label]])\n",
    "        return torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "    def len(self):\n",
    "        return self.dataframe.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        if self._data is None:\n",
    "            if self.test:\n",
    "                self._data = torch.load(os.path.join(self.processed_dir, 'data_test.pt'))\n",
    "            else:\n",
    "                self._data = torch.load(os.path.join(self.processed_dir, 'data.pt'))\n",
    "        return self._data[idx]\n",
    "\n",
    "## Customised preprocessing (Mols-To-Graph function)\n",
    "\n",
    "class Graph_custom(InMemoryDataset):\n",
    "    def __init__(self, dataframe, root, smiles_col='smiles', label_col='label', test=False, transform=None, pre_transform=None):\n",
    "        self.test = test\n",
    "        self.dataframe = dataframe\n",
    "        self.smiles_col = smiles_col\n",
    "        self.label_col = label_col\n",
    "        self._data = None\n",
    "        self.error_indices = []  # bad indices: To keep track of error indices\n",
    "        super(Graph_custom, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"\n",
    "        (The download func. is not implemented here)  \n",
    "        \"\"\"\n",
    "        return 'dataframe'\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\" If these files are found in raw_dir, processing is skipped\"\"\"\n",
    "        return ['data_test.pt' if self.test else 'data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        for index, mol in tqdm(self.dataframe.iterrows(), total=self.dataframe.shape[0]):\n",
    "            try:\n",
    "                mol_obj = Chem.MolFromSmiles(mol[self.smiles_col])\n",
    "                mol_obj = Chem.AddHs(mol_obj)\n",
    "                AllChem.EmbedMolecule(mol_obj, randomSeed=42, \n",
    "                                      useRandomCoords = True, maxAttempts = 5000 # Use this when Bad Conformer ID error\n",
    "                                     )\n",
    "                AllChem.MMFFOptimizeMolecule(mol_obj)\n",
    "                mol_obj = Chem.RemoveHs(mol_obj)\n",
    "                AllChem.ComputeGasteigerCharges(mol_obj)\n",
    "                \n",
    "                ################################################################\n",
    "                node_feats = self._get_node_features(mol_obj)\n",
    "                edge_feats = self._get_edge_features(mol_obj)\n",
    "                edge_index = self._get_adjacency_info(mol_obj)\n",
    "                label = self._get_labels(mol[self.label_col])\n",
    "\n",
    "                data = Data(x=node_feats, \n",
    "                            edge_index=edge_index,\n",
    "                            edge_attr=edge_feats,\n",
    "                            y=label,\n",
    "                            smiles=mol[self.smiles_col]\n",
    "                            ) \n",
    "                data_list.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing molecule at index {index}: {e}\")\n",
    "                self.error_indices.append(index)  # bad_indices: Track the index of the molecule that caused an error\n",
    "\n",
    "        if self.test:\n",
    "            torch.save(data_list, os.path.join(self.processed_dir, 'data_test.pt'))\n",
    "        else:\n",
    "            torch.save(data_list, os.path.join(self.processed_dir, 'data.pt'))\n",
    "\n",
    "    def _get_node_features(self, mol):\n",
    "        \"\"\" \n",
    "        This will return a matrix / 2d array of the shape\n",
    "        [Number of Nodes, Node Feature size]\n",
    "        \"\"\"\n",
    "        all_node_feats = []\n",
    "\n",
    "        for atom in mol.GetAtoms():\n",
    "            node_feats = []\n",
    "            # Feature 1: Atomic number        \n",
    "            node_feats.append(atom.GetAtomicNum())\n",
    "            # Feature 2: Atom degree -> Number of directly-bonded neighbours\n",
    "            node_feats.append(atom.GetDegree())\n",
    "            # Feature 3: Formal charge -> charge of the atom\n",
    "            node_feats.append(atom.GetFormalCharge())\n",
    "            # Feature 4: Hybridization -> hybridization state i.e. sp3\n",
    "            node_feats.append(atom.GetHybridization())\n",
    "            # Feature 5: Aromaticity\n",
    "            node_feats.append(atom.GetIsAromatic())\n",
    "            # Feature 6: Total Num Hs\n",
    "            node_feats.append(atom.GetTotalNumHs())\n",
    "            # Feature 7: Radical Electrons\n",
    "            node_feats.append(atom.GetNumRadicalElectrons())\n",
    "            # Feature 8: In Ring\n",
    "            node_feats.append(atom.IsInRing())\n",
    "            # Feature 9: Chirality\n",
    "            node_feats.append(atom.GetChiralTag())\n",
    "\n",
    "            # Feature 10: Gasteiger Charges\n",
    "            node_feats.append(atom.GetDoubleProp(\"_GasteigerCharge\"))\n",
    "            # Feature 11: Total Valence\n",
    "            node_feats.append(atom.GetTotalValence())\n",
    "            # Feature 12: Explicit Valence\n",
    "            node_feats.append(atom.GetExplicitValence())\n",
    "\n",
    "            # Append node features to matrix\n",
    "            all_node_feats.append(node_feats)\n",
    "\n",
    "        all_node_feats = np.asarray(all_node_feats)\n",
    "        return torch.tensor(all_node_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_edge_features(self, mol):\n",
    "        \"\"\" \n",
    "        This will return a matrix / 2d array of the shape\n",
    "        [Number of edges, Edge Feature size]\n",
    "        \"\"\"\n",
    "        conf = mol.GetConformer() # will be used to calculate bond length, force field optimised conformer\n",
    "        all_edge_feats = []\n",
    "        \n",
    "        for bond in mol.GetBonds():\n",
    "            edge_feats = []\n",
    "            # Feature 1: Bond type (as double)\n",
    "            edge_feats.append(bond.GetBondTypeAsDouble())\n",
    "            # Feature 2: Bond length\n",
    "            edge_feats.append(np.round(rdMolTransforms.GetBondLength(conf, bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()),3))\n",
    "            # Feature 3: Rings\n",
    "            edge_feats.append(bond.IsInRing())\n",
    "            # Append node features to matrix (twice, per direction)\n",
    "            all_edge_feats += [edge_feats, edge_feats]\n",
    "\n",
    "        all_edge_feats = np.asarray(all_edge_feats)\n",
    "        return torch.tensor(all_edge_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_adjacency_info(self, mol):\n",
    "        edge_indices = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            edge_indices += [[i, j], [j, i]]\n",
    "\n",
    "        edge_indices = torch.tensor(edge_indices)\n",
    "        edge_indices = edge_indices.t().to(torch.long).view(2, -1)\n",
    "        return edge_indices\n",
    "\n",
    "    def _get_labels(self, label):\n",
    "        label = np.asarray([[label]])\n",
    "        return torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "    def len(self):\n",
    "        return self.dataframe.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        if self._data is None: \n",
    "            if self.test:\n",
    "                self._data = torch.load(os.path.join(self.processed_dir, 'data_test.pt'))\n",
    "            else:\n",
    "                self._data = torch.load(os.path.join(self.processed_dir, 'data.pt'))\n",
    "        return self._data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28316b93-eb27-477b-bfeb-a2166c7dd77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self, conv_type, hidden_channels, num_features, batch_norm=False, weight_init=True, dropout_rate=0.0):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = conv_type(num_features, hidden_channels*2)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels*2) if batch_norm else None #BatchNorm1D -> GraphNorm\n",
    "        \n",
    "        self.conv2 = conv_type(hidden_channels*2, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels) if batch_norm else None\n",
    "        \n",
    "        self.conv3 = conv_type(hidden_channels, hidden_channels//2)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "        self.lin = torch.nn.Linear(hidden_channels//2, 1)\n",
    "        if weight_init:\n",
    "            self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, torch_geometric.nn.MessagePassing):\n",
    "            for param in module.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    torch.nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        if self.bn1:\n",
    "            x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "            \n",
    "        x = self.conv2(x, edge_index)\n",
    "        if self.bn2:\n",
    "            x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "            \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = torch_geometric.nn.global_mean_pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "class GINModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_features, batch_norm=False, weight_init=True, dropout_rate=0.0):\n",
    "        super(GINModel, self).__init__()\n",
    "        # Define MLP for GINConv\n",
    "        def mlp(input_dim, output_dim):\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_dim, output_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(output_dim, output_dim)\n",
    "            )\n",
    "\n",
    "        self.conv1 = torch_geometric.nn.GINConv(mlp(num_features, hidden_channels*2))\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels*2) if batch_norm else None\n",
    "        \n",
    "        self.conv2 = torch_geometric.nn.GINConv(mlp(hidden_channels*2, hidden_channels))\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels) if batch_norm else None\n",
    "        \n",
    "        self.conv3 = torch_geometric.nn.GINConv(mlp(hidden_channels, hidden_channels//2))\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "        self.lin = torch.nn.Linear(hidden_channels//2, 1)\n",
    "\n",
    "        if weight_init:\n",
    "            self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, GINConv):\n",
    "            for layer in module.nn:\n",
    "                if isinstance(layer, torch.nn.Linear):\n",
    "                    torch.nn.init.xavier_uniform_(layer.weight)\n",
    "                    if layer.bias is not None:\n",
    "                        torch.nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        if self.bn1:\n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "        x = self.conv2(x, edge_index)\n",
    "        if self.bn2:\n",
    "            x = self.bn2(x)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "        x = self.conv3(x, edge_index)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = torch_geometric.nn.global_mean_pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "def train(model, loader, optimiser, criterion, device, threshold = 0.5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y.view(-1, 1).float())\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = (out > threshold).float()\n",
    "        correct += pred.eq(data.y.view(-1, 1)).sum().item()\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion, device, threshold = 0.5):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y.view(-1, 1).float())\n",
    "            total_loss += loss.item()\n",
    "            pred = (out > threshold).float()\n",
    "            correct += pred.eq(data.y.view(-1, 1)).sum().item()\n",
    "            preds.append(out)\n",
    "            labels.append(data.y.view(-1, 1))\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    return total_loss / len(loader), correct / len(loader.dataset), roc_auc_score(labels.cpu(), preds.cpu()), f1_score(labels.cpu(), (preds > threshold).float().cpu()), matthews_corrcoef(labels.cpu(), (preds > threshold).float().cpu())\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_score, model):\n",
    "        score = -val_score\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_score, model)\n",
    "        elif score < self.best_score - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_score, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_score, model):\n",
    "        self.best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69f3440a-d7e7-4e14-a1ae-d3828bed69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_indices(pytorch_custom_dataset, seed=42):\n",
    "    num_samples = len(pytorch_custom_dataset)\n",
    "    indices = np.arange(num_samples)\n",
    "    targets = [data.y.item() for data in pytorch_custom_dataset]\n",
    "\n",
    "    skf_outer = StratifiedKFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "    outer_folds = []\n",
    "\n",
    "    for train_indices, test_indices in skf_outer.split(indices, targets):\n",
    "        train_targets = np.array(targets)[train_indices]\n",
    "        skf_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        inner_folds = []\n",
    "        for inner_train_idx, val_idx in skf_inner.split(np.arange(len(train_indices)), train_targets):\n",
    "            inner_train_indices = train_indices[inner_train_idx]\n",
    "            val_indices = train_indices[val_idx]\n",
    "            inner_folds.append((inner_train_indices, val_indices))\n",
    "        outer_folds.append((train_indices, test_indices, inner_folds)) \n",
    "    return outer_folds\n",
    "\n",
    "def objective(hyperparameters, model_class, conv_type, pytorch_custom_dataset, device, results_dir, model_name, outer_fold, inner_fold, fold_indices, use_early_stopping):\n",
    "    learning_rate = hyperparameters['learning_rate']\n",
    "    hidden_channels = hyperparameters['hidden_channels']\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    dropout_rate = hyperparameters['dropout_rate']\n",
    "    num_epochs = 100 \n",
    "\n",
    "    torch.manual_seed(1)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1)\n",
    "\n",
    "    train_indices, test_indices, inner_folds = fold_indices[outer_fold]\n",
    "    inner_train_indices, val_indices = inner_folds[inner_fold]\n",
    "\n",
    "    train_dataset = Subset(pytorch_custom_dataset, inner_train_indices)\n",
    "    val_dataset = Subset(pytorch_custom_dataset, val_indices)\n",
    "    test_dataset = Subset(pytorch_custom_dataset, test_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    num_features = pytorch_custom_dataset[0].num_features \n",
    "\n",
    "    if model_class == GINModel:\n",
    "        model = model_class(\n",
    "            hidden_channels=hidden_channels,\n",
    "            num_features=num_features,\n",
    "            batch_norm=False,\n",
    "            weight_init=True,\n",
    "            dropout_rate=dropout_rate\n",
    "        ).to(device)\n",
    "    else:\n",
    "        model = model_class(\n",
    "            conv_type=conv_type,\n",
    "            hidden_channels=hidden_channels,\n",
    "            num_features=num_features,\n",
    "            batch_norm=False,\n",
    "            weight_init=True,\n",
    "            dropout_rate=dropout_rate\n",
    "        ).to(device)\n",
    "\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    early_stopping = EarlyStopping(patience = 10, delta=0)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_acc = train(model, train_loader, optimiser, criterion, device)\n",
    "        val_loss, val_acc, val_auc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        if use_early_stopping:\n",
    "            early_stopping(val_auc, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "    best_model = early_stopping.best_model if use_early_stopping else model\n",
    "    val_loss, val_acc, val_auc, _, _ = evaluate(best_model, val_loader, criterion, device)\n",
    "    \n",
    "    return val_auc\n",
    "\n",
    "def grid_search(model_class, conv_type, pytorch_custom_dataset, results_dir, model_name, use_early_stopping):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    fold_indices = get_fold_indices(pytorch_custom_dataset, seed=42)\n",
    "\n",
    "    batch_size_list = [32, 64, 128]\n",
    "    hidden_channels_list = [64, 128, 256]\n",
    "    learning_rate_list = [0.001, 0.0001, 0.00001]\n",
    "    dropout_rate_list = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "    hyperparameter_grid = list(itertools.product(\n",
    "        batch_size_list,\n",
    "        hidden_channels_list,\n",
    "        learning_rate_list,\n",
    "        dropout_rate_list\n",
    "    ))\n",
    "\n",
    "    results = []\n",
    "    for outer_fold in range(4):\n",
    "        print(f\"Outer Fold {outer_fold + 1}/4\")\n",
    "\n",
    "        for inner_fold in range(5):\n",
    "            print(f\"  Inner Fold {inner_fold + 1}/5\")\n",
    "\n",
    "            for batch_size, hidden_channels, learning_rate, dropout_rate in hyperparameter_grid:\n",
    "                hyperparameters_dict = {\n",
    "                    'batch_size': batch_size,\n",
    "                    'hidden_channels': hidden_channels,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'dropout_rate': dropout_rate\n",
    "                }\n",
    "\n",
    "                val_auc = objective(\n",
    "                    hyperparameters=hyperparameters_dict,\n",
    "                    model_class=model_class,\n",
    "                    conv_type=conv_type,\n",
    "                    pytorch_custom_dataset=pytorch_custom_dataset,\n",
    "                    device=device,\n",
    "                    results_dir=results_dir,\n",
    "                    model_name=model_name,\n",
    "                    outer_fold=outer_fold,\n",
    "                    inner_fold=inner_fold,\n",
    "                    fold_indices=fold_indices,\n",
    "                    use_early_stopping = use_early_stopping\n",
    "                )\n",
    "                print(f\"Testing hyperparameters: bs={batch_size}, hc={hidden_channels}, lr={learning_rate}, dr={dropout_rate} / val_auc: {val_auc}\")\n",
    "                \n",
    "                result = {\n",
    "                    'model_name': model_name,\n",
    "                    'outer_fold': outer_fold,\n",
    "                    'inner_fold': inner_fold,\n",
    "                    'batch_size': batch_size,\n",
    "                    'hidden_channels': hidden_channels,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'val_auc': val_auc\n",
    "                }\n",
    "\n",
    "                results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_grid_search(models, pytorch_custom_dataset, results_dir, use_early_stopping):\n",
    "    all_results = []\n",
    "\n",
    "    for name, model_class in models.items():\n",
    "        print(f\"Running Grid Search for {name} model...\")\n",
    "\n",
    "        if \"GCN\" in name:\n",
    "            conv_type = GCNConv\n",
    "        elif \"GAT\" in name:\n",
    "            conv_type = GATConv\n",
    "        elif \"GraphSAGE\" in name:\n",
    "            conv_type = SAGEConv\n",
    "        else:\n",
    "            conv_type = None \n",
    "\n",
    "        results = grid_search(\n",
    "            model_class=model_class,\n",
    "            conv_type=conv_type,\n",
    "            pytorch_custom_dataset=pytorch_custom_dataset,\n",
    "            results_dir=results_dir,\n",
    "            model_name=name,\n",
    "            use_early_stopping = use_early_stopping\n",
    "        )\n",
    "\n",
    "        all_results.extend(results)\n",
    "\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc215c6-237d-46a4-83a6-fa0d535267a2",
   "metadata": {},
   "source": [
    "#### Drug-induced liver injury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f096e3-d655-48d4-8eda-5d9a8ee26488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated from DILIGeNN Data Preprocessing.ipynb\n",
    "df_dili_filtered_cleaned = pd.read_csv('chem_data/DILI/DILIst_standardised_cleaned.csv', index_col = 0)\n",
    "\n",
    "dlst_min = Graph_basic(dataframe = df_dili_filtered_cleaned,\n",
    "                              smiles_col = 'smiles',\n",
    "                             label_col = 'label',\n",
    "                             root = 'custom_data/minimal_feature/dlst_min'\n",
    "                            )\n",
    "dlst_cus_2_std = Graph_custom(dataframe = df_dili_filtered_cleaned,\n",
    "                              smiles_col = 'smiles_std',\n",
    "                             label_col = 'label',\n",
    "                             root = 'custom_data/DILI/cus2_std/'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e1ce4-25e6-415e-82ee-93174d8f8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlst_cus_2_std\n",
    "models = {\n",
    "    'GCN_Optimised': GNNModel,\n",
    "    'GAT_Optimised': GNNModel,\n",
    "    'GraphSAGE_Optimised': GNNModel,\n",
    "    'GIN_Optimised': GINModel\n",
    "}\n",
    "\n",
    "results_dir = './results/grid_search/dlst'\n",
    "\n",
    "# Run Grid Search\n",
    "all_results = run_grid_search(\n",
    "    models,\n",
    "    dlst_cus_2_std,\n",
    "    results_dir,\n",
    "    use_early_stopping = True\n",
    ")\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(os.path.join(results_dir, 'grid_search_results.csv'), index=False)\n",
    "\n",
    "with open(os.path.join(results_dir, 'grid_search_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e9d63-35a2-4466-b58d-8872e6191b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlst_min\n",
    "models = {\n",
    "    'GCN_Optimised': GNNModel,\n",
    "    'GAT_Optimised': GNNModel,\n",
    "    'GraphSAGE_Optimised': GNNModel,\n",
    "    'GIN_Optimised': GINModel\n",
    "}\n",
    "\n",
    "results_dir = './results/grid_search/dlst_min'\n",
    "\n",
    "# Run Grid Search\n",
    "all_results = run_grid_search(\n",
    "    models,\n",
    "    dlst_min,\n",
    "    results_dir,\n",
    "    use_early_stopping = True\n",
    ")\n",
    "\n",
    "# Create DataFrame \n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save DataFrame\n",
    "results_df.to_csv(os.path.join(results_dir, 'grid_search_results.csv'), index=False)\n",
    "\n",
    "# Optionally, save the DataFrame as a pickle file\n",
    "with open(os.path.join(results_dir, 'grid_search_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d5e42-201f-4b9c-8342-34527f6f9daf",
   "metadata": {},
   "source": [
    "#### Blood-Brain Barrier Permeability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398904f5-2ee6-4bee-96ed-aeeac615c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bbbp_filtered_cleaned = pd.read_csv('chem_data/bbbp_standardised_cleaned.csv', index_col = 0)\n",
    "\n",
    "bbbp_min = Graph_basic(dataframe = df_bbbp_filtered_cleaned,\n",
    "                              smiles_col = 'smiles',\n",
    "                             label_col = 'label',\n",
    "                             root = 'custom_data/minimal_feature/bbbp_min'\n",
    "                            )\n",
    "bbbp_cus_2_std = Graph_custom(dataframe = df_bbbp_filtered_cleaned,\n",
    "                              smiles_col = 'smiles_std',\n",
    "                             label_col = 'label',\n",
    "                             root = 'custom_data/bbbp/cus2_std/'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cde34d-b760-4b69-a964-a75e845335c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbbp_cus_2_std\n",
    "models = {\n",
    "    'GCN_Optimised': GNNModel,\n",
    "    'GAT_Optimised': GNNModel,\n",
    "    'GraphSAGE_Optimised': GNNModel,\n",
    "    'GIN_Optimised': GINModel\n",
    "}\n",
    "\n",
    "results_dir = './results/grid_search/bbbp'\n",
    "\n",
    "all_results = run_grid_search(\n",
    "    models,\n",
    "    bbbp_cus_2_std,\n",
    "    results_dir,\n",
    "    use_early_stopping = True\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(os.path.join(results_dir, 'grid_search_results.csv'), index=False)\n",
    "\n",
    "with open(os.path.join(results_dir, 'grid_search_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc3407-f187-424c-8fe9-9232072df248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbbp_min\n",
    "models = {\n",
    "    'GCN_Optimised': GNNModel,\n",
    "    'GAT_Optimised': GNNModel,\n",
    "    'GraphSAGE_Optimised': GNNModel,\n",
    "    'GIN_Optimised': GINModel\n",
    "}\n",
    "\n",
    "results_dir = './results/grid_search/bbbp_min'\n",
    "\n",
    "all_results = run_grid_search(\n",
    "    models,\n",
    "    bbbp_min,\n",
    "    results_dir,\n",
    "    use_early_stopping = True\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(os.path.join(results_dir, 'grid_search_results.csv'), index=False)\n",
    "\n",
    "with open(os.path.join(results_dir, 'grid_search_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec345438-5aa4-4f6e-92c2-0cca8d19ba77",
   "metadata": {},
   "source": [
    "#### BACE Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3b8e4-e3ec-400b-a9c7-8b32fd6bb966",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bace_filtered_cleaned = pd.read_csv('chem_data/bace_standardised_cleaned.csv', index_col = 0)\n",
    "\n",
    "bace_min = Graph_basic(dataframe = df_bace_filtered_cleaned,\n",
    "                              smiles_col = 'smiles',\n",
    "                             label_col = 'label',\n",
    "                             root = 'custom_data/minimal_feature/bace_min'\n",
    "                            )\n",
    "bace_cus_2_std = Graph_custom(dataframe = df_bace_filtered_cleaned,\n",
    "                              smiles_col = 'smiles_std',\n",
    "                             label_col = 'label',\n",
    "                             root = 'custom_data/bace/cus2_std/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643137d8-0c3c-4b52-bd66-911152630f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bace_cus_2_std\n",
    "models = {\n",
    "    'GCN_Optimised': GNNModel,\n",
    "    'GAT_Optimised': GNNModel,\n",
    "    'GraphSAGE_Optimised': GNNModel,\n",
    "    'GIN_Optimised': GINModel\n",
    "}\n",
    "\n",
    "results_dir = './results/grid_search/bace'\n",
    "\n",
    "all_results = run_grid_search(\n",
    "    models,\n",
    "    bace_cus_2_std,\n",
    "    results_dir,\n",
    "    use_early_stopping = True\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(os.path.join(results_dir, 'grid_search_results.csv'), index=False)\n",
    "\n",
    "with open(os.path.join(results_dir, 'grid_search_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b378a-5b2e-4d3a-9770-1b6a75612556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bace_min\n",
    "models = {\n",
    "    'GCN_Optimised': GNNModel,\n",
    "    'GAT_Optimised': GNNModel,\n",
    "    'GraphSAGE_Optimised': GNNModel,\n",
    "    'GIN_Optimised': GINModel\n",
    "}\n",
    "\n",
    "results_dir = './results/grid_search/bace_min'\n",
    "\n",
    "all_results = run_grid_search(\n",
    "    models,\n",
    "    bace_min,\n",
    "    results_dir,\n",
    "    use_early_stopping = True\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(os.path.join(results_dir, 'grid_search_results.csv'), index=False)\n",
    "\n",
    "with open(os.path.join(results_dir, 'grid_search_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2bd546-31db-4e25-af13-2b90012ba479",
   "metadata": {},
   "source": [
    "#### ClinTox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a5c3e-71ea-44fd-9e8e-0d6399dfc7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph_custom(InMemoryDataset):\n",
    "    def __init__(self, dataframe, root, smiles_col='smiles', label_col='label', test=False, transform=None, pre_transform=None):\n",
    "        self.test = test\n",
    "        self.dataframe = dataframe\n",
    "        self.smiles_col = smiles_col\n",
    "        self.label_col = label_col\n",
    "        self._data = None\n",
    "        self.error_indices = []\n",
    "        super(Graph_custom, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"\n",
    "        (The download func. is not implemented here)  \n",
    "        \"\"\"\n",
    "        return 'dataframe'\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"If these files are found in raw_dir, processing is skipped\"\"\"\n",
    "        return ['data_test.pt' if self.test else 'data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        for index, mol in tqdm(self.dataframe.iterrows(), total=self.dataframe.shape[0]):\n",
    "            try:\n",
    "                mol_obj = Chem.MolFromSmiles(mol[self.smiles_col])\n",
    "\n",
    "                mol_obj = Chem.AddHs(mol_obj)\n",
    "                AllChem.EmbedMolecule(mol_obj, randomSeed=42, \n",
    "                                      useRandomCoords=True, maxAttempts=5000)\n",
    "                AllChem.MMFFOptimizeMolecule(mol_obj)\n",
    "                mol_obj = Chem.RemoveHs(mol_obj)\n",
    "                AllChem.ComputeGasteigerCharges(mol_obj)\n",
    "\n",
    "                node_feats = self._get_node_features(mol_obj)\n",
    "                edge_feats = self._get_edge_features(mol_obj)\n",
    "                edge_index = self._get_adjacency_info(mol_obj)\n",
    "                \n",
    "                label = self._get_labels(mol[self.label_col])\n",
    "\n",
    "                data = Data(x=node_feats, \n",
    "                            edge_index=edge_index,\n",
    "                            edge_attr=edge_feats,\n",
    "                            y=label,\n",
    "                            smiles=mol[self.smiles_col])\n",
    "                data_list.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing molecule at index {index}: {e}\")\n",
    "                self.error_indices.append(index)\n",
    "\n",
    "        if self.test:\n",
    "            torch.save(data_list, os.path.join(self.processed_dir, 'data_test.pt'))\n",
    "        else:\n",
    "            torch.save(data_list, os.path.join(self.processed_dir, 'data.pt'))\n",
    "\n",
    "    def _get_node_features(self, mol):\n",
    "        \"\"\" \n",
    "        Returns a matrix / 2d array of the shape\n",
    "        [Number of Nodes, Node Feature size]\n",
    "        \"\"\"\n",
    "        all_node_feats = []\n",
    "\n",
    "        for atom in mol.GetAtoms():\n",
    "            node_feats = []\n",
    "            # Feature 1: Atomic number        \n",
    "            node_feats.append(atom.GetAtomicNum())\n",
    "            # Feature 2: Atom degree -> Number of directly-bonded neighbours\n",
    "            node_feats.append(atom.GetDegree())\n",
    "            # Feature 3: Formal charge -> charge of the atom\n",
    "            node_feats.append(atom.GetFormalCharge())\n",
    "            # Feature 4: Hybridization -> hybridization state i.e. sp3\n",
    "            node_feats.append(int(atom.GetHybridization()))\n",
    "            # Feature 5: Aromaticity\n",
    "            node_feats.append(int(atom.GetIsAromatic()))\n",
    "            # Feature 6: Total Num Hs\n",
    "            node_feats.append(atom.GetTotalNumHs())\n",
    "            # Feature 7: Radical Electrons\n",
    "            node_feats.append(atom.GetNumRadicalElectrons())\n",
    "            # Feature 8: In Ring\n",
    "            node_feats.append(int(atom.IsInRing()))\n",
    "            # Feature 9: Chirality\n",
    "            node_feats.append(int(atom.GetChiralTag()))\n",
    "\n",
    "            # Feature 10: Gasteiger Charges\n",
    "            node_feats.append(atom.GetDoubleProp(\"_GasteigerCharge\"))\n",
    "            # Feature 11: Total Valence\n",
    "            node_feats.append(atom.GetTotalValence())\n",
    "            # Feature 12: Explicit Valence\n",
    "            node_feats.append(atom.GetExplicitValence())\n",
    "\n",
    "            # Append node features to matrix\n",
    "            all_node_feats.append(node_feats)\n",
    "\n",
    "        all_node_feats = np.asarray(all_node_feats)\n",
    "        return torch.tensor(all_node_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_edge_features(self, mol):\n",
    "        \"\"\" \n",
    "        Returns a matrix / 2d array of the shape\n",
    "        [Number of edges, Edge Feature size]\n",
    "        \"\"\"\n",
    "        conf = mol.GetConformer()\n",
    "        all_edge_feats = []\n",
    "        \n",
    "        for bond in mol.GetBonds():\n",
    "            edge_feats = []\n",
    "            # Feature 1: Bond type (as double)\n",
    "            edge_feats.append(bond.GetBondTypeAsDouble())\n",
    "            # Feature 2: Bond length\n",
    "            edge_feats.append(np.round(rdMolTransforms.GetBondLength(conf, bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()), 3))\n",
    "            # Feature 3: Rings\n",
    "            edge_feats.append(int(bond.IsInRing()))\n",
    "            # Append edge features to matrix (twice, per direction)\n",
    "            all_edge_feats += [edge_feats, edge_feats]\n",
    "\n",
    "        all_edge_feats = np.asarray(all_edge_feats)\n",
    "        return torch.tensor(all_edge_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_adjacency_info(self, mol):\n",
    "        edge_indices = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            edge_indices += [[i, j], [j, i]]\n",
    "\n",
    "        edge_indices = torch.tensor(edge_indices)\n",
    "        edge_indices = edge_indices.t().to(torch.long).view(2, -1)\n",
    "        return edge_indices\n",
    "\n",
    "    def _get_labels(self, label):\n",
    "        if isinstance(label, str):\n",
    "            label = ast.literal_eval(label)\n",
    "        label = np.array(label, dtype=np.int64) \n",
    "        return torch.tensor(label, dtype=torch.int64) \n",
    "\n",
    "    def len(self):\n",
    "        return self.dataframe.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        if self._data is None:\n",
    "            if self.test:\n",
    "                self._data = torch.load(os.path.join(self.processed_dir, 'data_test.pt'))\n",
    "            else:\n",
    "                self._data = torch.load(os.path.join(self.processed_dir, 'data.pt'))\n",
    "        return self._data[idx]\n",
    "\n",
    "class Graph_basic(InMemoryDataset):\n",
    "    def __init__(self, dataframe, root, smiles_col='smiles', label_col='label', test=False, transform=None, pre_transform=None):\n",
    "        self.test = test\n",
    "        self.dataframe = dataframe\n",
    "        self.smiles_col = smiles_col\n",
    "        self.label_col = label_col\n",
    "        self._data = None\n",
    "        self.error_indices = []\n",
    "        super(Graph_basic, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return 'dataframe'\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data_test.pt' if self.test else 'data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        for index, mol in tqdm(self.dataframe.iterrows(), total=self.dataframe.shape[0]):\n",
    "            try:\n",
    "                mol_obj = Chem.MolFromSmiles(mol[self.smiles_col])\n",
    "\n",
    "                # Get node features\n",
    "                node_feats = self._get_node_features(mol_obj)\n",
    "                # Get edge features\n",
    "                edge_feats = self._get_edge_features(mol_obj)\n",
    "                # Get adjacency info\n",
    "                edge_index = self._get_adjacency_info(mol_obj)\n",
    "                # Get labels info\n",
    "                label = self._get_labels(mol[self.label_col])\n",
    "\n",
    "                # Create data object\n",
    "                data = Data(x=node_feats, \n",
    "                            edge_index=edge_index,\n",
    "                            edge_attr=edge_feats,\n",
    "                            y=label,\n",
    "                            smiles=mol[self.smiles_col])\n",
    "                data_list.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing molecule at index {index}: {e}\")\n",
    "                self.error_indices.append(index)\n",
    "\n",
    "        # Save all data objects into one file\n",
    "        if self.test:\n",
    "            torch.save(data_list, os.path.join(self.processed_dir, 'data_test.pt'))\n",
    "        else:\n",
    "            torch.save(data_list, os.path.join(self.processed_dir, 'data.pt'))\n",
    "\n",
    "    def _get_node_features(self, mol):\n",
    "        \"\"\" \n",
    "        Return a matrix / 2D array of the shape [Number of Nodes, Node Feature size]\n",
    "        with atomic number as the only node feature.\n",
    "        \"\"\"\n",
    "        all_node_feats = []\n",
    "\n",
    "        for atom in mol.GetAtoms():\n",
    "            node_feats = []\n",
    "            # Feature: Atomic number        \n",
    "            node_feats.append(atom.GetAtomicNum())\n",
    "            # Append node features to matrix\n",
    "            all_node_feats.append(node_feats)\n",
    "\n",
    "        all_node_feats = np.asarray(all_node_feats)\n",
    "        return torch.tensor(all_node_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_edge_features(self, mol):\n",
    "        \"\"\" \n",
    "        Return a matrix / 2D array of the shape [Number of edges, Edge Feature size]\n",
    "        with bond type as the only edge feature.\n",
    "        \"\"\"\n",
    "        all_edge_feats = []\n",
    "        \n",
    "        for bond in mol.GetBonds():\n",
    "            edge_feats = []\n",
    "            # Feature: Bond type (as double)\n",
    "            edge_feats.append(bond.GetBondTypeAsDouble())\n",
    "            # Append edge features to matrix (twice, per direction)\n",
    "            all_edge_feats += [edge_feats, edge_feats]\n",
    "\n",
    "        all_edge_feats = np.asarray(all_edge_feats)\n",
    "        return torch.tensor(all_edge_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_adjacency_info(self, mol):\n",
    "        edge_indices = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            edge_indices += [[i, j], [j, i]]\n",
    "\n",
    "        edge_indices = torch.tensor(edge_indices)\n",
    "        edge_indices = edge_indices.t().to(torch.long).view(2, -1)\n",
    "        return edge_indices\n",
    "\n",
    "    def _get_labels(self, label):\n",
    "        if isinstance(label, str):\n",
    "            label = ast.literal_eval(label)\n",
    "        label = np.array(label, dtype=np.int64)\n",
    "        return torch.tensor(label, dtype=torch.int64) \n",
    "\n",
    "    def len(self):\n",
    "        return self.dataframe.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        if self._data is None:\n",
    "            if self.test:\n",
    "                self._data = torch.load(os.path.join(self.processed_dir, 'data_test.pt'))\n",
    "            else:\n",
    "                self._data = torch.load(os.path.join(self.processed_dir, 'data.pt'))\n",
    "        return self._data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8cccb1-5bd0-4d32-bbce-17dcbc08aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Classes\n",
    "\n",
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self, conv_type, hidden_channels, num_features, num_classes=3, batch_norm=False, weight_init=True, dropout_rate=0.0):  # changed labels\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = conv_type(num_features, hidden_channels*2)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels*2) if batch_norm else None\n",
    "\n",
    "        self.conv2 = conv_type(hidden_channels*2, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels) if batch_norm else None\n",
    "\n",
    "        self.conv3 = conv_type(hidden_channels, hidden_channels//2)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "        self.lin = torch.nn.Linear(hidden_channels//2, num_classes)  # Adjusted output layer for multi-class classification\n",
    "        if weight_init:\n",
    "            self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, torch_geometric.nn.MessagePassing):\n",
    "            for param in module.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    torch.nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        if self.bn1:\n",
    "            x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        if self.bn2:\n",
    "            x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = torch_geometric.nn.global_mean_pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x  # Outputs logits for each class\n",
    "\n",
    "class GINModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_features, num_classes=3, batch_norm=False, weight_init=True, dropout_rate=0.0):  # changed labels\n",
    "        super(GINModel, self).__init__()\n",
    "        # Define MLP for GINConv\n",
    "        def mlp(input_dim, output_dim):\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_dim, output_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(output_dim, output_dim)\n",
    "            )\n",
    "\n",
    "        self.conv1 = torch_geometric.nn.GINConv(mlp(num_features, hidden_channels*2))\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels*2) if batch_norm else None\n",
    "\n",
    "        self.conv2 = torch_geometric.nn.GINConv(mlp(hidden_channels*2, hidden_channels))\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels) if batch_norm else None\n",
    "\n",
    "        self.conv3 = torch_geometric.nn.GINConv(mlp(hidden_channels, hidden_channels//2))\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "        self.lin = torch.nn.Linear(hidden_channels//2, num_classes)  # Adjusted output layer for multi-class classification\n",
    "\n",
    "        if weight_init:\n",
    "            self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, GINConv):\n",
    "            for layer in module.nn:\n",
    "                if isinstance(layer, torch.nn.Linear):\n",
    "                    torch.nn.init.xavier_uniform_(layer.weight)\n",
    "                    if layer.bias is not None:\n",
    "                        torch.nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        if self.bn1:\n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        if self.bn2:\n",
    "            x = self.bn2(x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = torch_geometric.nn.global_mean_pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x  \n",
    "\n",
    "\n",
    "# Training and Evaluation Functions\n",
    "\n",
    "def train(model, loader, optimiser, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y.squeeze()) # Use CrossEntropyLoss with class indices\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += pred.eq(data.y.squeeze()).sum().item()\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader), accuracy\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y.squeeze()) # Use CrossEntropyLoss with class indices\n",
    "            total_loss += loss.item()\n",
    "            pred = out.argmax(dim=1)  # changed labels\n",
    "            correct += pred.eq(data.y.squeeze()).sum().item()\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(data.y.squeeze().cpu().numpy())\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "\n",
    "    try:\n",
    "        all_probs = []\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            probs = F.softmax(out, dim=1)\n",
    "            all_probs.append(probs.cpu().detach())\n",
    "        all_probs = torch.cat(all_probs, dim=0).numpy()\n",
    "        auc = roc_auc_score(labels, all_probs, multi_class='ovr', average='macro')\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    return total_loss / len(loader), accuracy, auc, f1, mcc\n",
    "\n",
    "\n",
    "# Early Stopping Class\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_score, model):\n",
    "        score = -val_score\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_score, model)\n",
    "        elif score < self.best_score - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_score, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_score, model):\n",
    "        self.best_model = model\n",
    "\n",
    "\n",
    "# Cross-validation and Grid search\n",
    "\n",
    "def get_fold_indices(pytorch_custom_dataset, seed=42):\n",
    "    num_samples = len(pytorch_custom_dataset)\n",
    "    indices = np.arange(num_samples)\n",
    "    targets = np.array([data.y.item() for data in pytorch_custom_dataset])\n",
    "\n",
    "    skf_outer = StratifiedKFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "    outer_folds = []\n",
    "\n",
    "    for train_indices, test_indices in skf_outer.split(indices, targets):\n",
    "        train_targets = targets[train_indices]\n",
    "        skf_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        inner_folds = []\n",
    "        for inner_train_idx, val_idx in skf_inner.split(train_indices, train_targets):\n",
    "            inner_train_indices = train_indices[inner_train_idx]\n",
    "            val_indices = train_indices[val_idx]\n",
    "            inner_folds.append((inner_train_indices, val_indices))\n",
    "        outer_folds.append((train_indices, test_indices, inner_folds))\n",
    "    return outer_folds\n",
    "\n",
    "def objective(hyperparameters, model_class, conv_type, pytorch_custom_dataset, device, results_dir, model_name, outer_fold, inner_fold, fold_indices, use_early_stopping):\n",
    "    # Extract hyperparameters\n",
    "    learning_rate = hyperparameters['learning_rate']\n",
    "    hidden_channels = hyperparameters['hidden_channels']\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    dropout_rate = hyperparameters['dropout_rate']\n",
    "    num_epochs = 100\n",
    "\n",
    "    torch.manual_seed(1)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1)\n",
    "\n",
    "    train_indices, test_indices, inner_folds = fold_indices[outer_fold]\n",
    "    inner_train_indices, val_indices = inner_folds[inner_fold]\n",
    "\n",
    "    train_dataset = Subset(pytorch_custom_dataset, inner_train_indices)\n",
    "    val_dataset = Subset(pytorch_custom_dataset, val_indices)\n",
    "    test_dataset = Subset(pytorch_custom_dataset, test_indices)\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    num_features = pytorch_custom_dataset[0].num_features\n",
    "    num_classes = 3\n",
    "\n",
    "    # Init models\n",
    "    if model_class == GINModel:\n",
    "        model = model_class(\n",
    "            hidden_channels=hidden_channels,\n",
    "            num_features=num_features,\n",
    "            num_classes=num_classes,\n",
    "            batch_norm=False,\n",
    "            weight_init=True,\n",
    "            dropout_rate=dropout_rate\n",
    "        ).to(device)\n",
    "    else:\n",
    "        model = model_class(\n",
    "            conv_type=conv_type,\n",
    "            hidden_channels=hidden_channels,\n",
    "            num_features=num_features,\n",
    "            num_classes=num_classes,\n",
    "            batch_norm=False,\n",
    "            weight_init=True,\n",
    "            dropout_rate=dropout_rate\n",
    "        ).to(device)\n",
    "\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()  # CE\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    early_stopping = EarlyStopping(patience=10, delta=0)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_acc = train(model, train_loader, optimiser, criterion, device)\n",
    "        val_loss, val_acc, val_auc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        if use_early_stopping:\n",
    "            early_stopping(val_auc, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "    best_model = early_stopping.best_model if use_early_stopping else model\n",
    "    val_loss, val_acc, val_auc, _, _ = evaluate(best_model, val_loader, criterion, device)\n",
    "    \n",
    "    return val_auc\n",
    "\n",
    "def grid_search(model_class, conv_type, pytorch_custom_dataset, results_dir, model_name, use_early_stopping):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Indices for train / val / test data\n",
    "    fold_indices = get_fold_indices(pytorch_custom_dataset, seed=42)\n",
    "\n",
    "    # hyperparameter grid\n",
    "    batch_size_list = [32, 64, 128]\n",
    "    hidden_channels_list = [64, 128, 256]\n",
    "    learning_rate_list = [0.001, 0.0001, 0.00001]\n",
    "    dropout_rate_list = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "    hyperparameter_grid = list(itertools.product(\n",
    "        batch_size_list,\n",
    "        hidden_channels_list,\n",
    "        learning_rate_list,\n",
    "        dropout_rate_list\n",
    "    ))\n",
    "\n",
    "    # List to store results\n",
    "    results = []\n",
    "\n",
    "    for outer_fold in range(4):\n",
    "        print(f\"Outer Fold {outer_fold + 1}/4\")\n",
    "\n",
    "        for inner_fold in range(5):\n",
    "            print(f\"  Inner Fold {inner_fold + 1}/5\")\n",
    "\n",
    "            for batch_size, hidden_channels, learning_rate, dropout_rate in hyperparameter_grid:\n",
    "                hyperparameters_dict = {\n",
    "                    'batch_size': batch_size,\n",
    "                    'hidden_channels': hidden_channels,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'dropout_rate': dropout_rate\n",
    "                }\n",
    "\n",
    "                val_auc = objective(\n",
    "                    hyperparameters=hyperparameters_dict,\n",
    "                    model_class=model_class,\n",
    "                    conv_type=conv_type,\n",
    "                    pytorch_custom_dataset=pytorch_custom_dataset,\n",
    "                    device=device,\n",
    "                    results_dir=results_dir,\n",
    "                    model_name=model_name,\n",
    "                    outer_fold=outer_fold,\n",
    "                    inner_fold=inner_fold,\n",
    "                    fold_indices=fold_indices,\n",
    "                    use_early_stopping=use_early_stopping\n",
    "                )\n",
    "                print(f\"Testing hyperparameters: bs={batch_size}, hc={hidden_channels}, lr={learning_rate}, dr={dropout_rate} / val_auc: {val_auc}\")\n",
    "                \n",
    "                # Store the results\n",
    "                result = {\n",
    "                    'model_name': model_name,\n",
    "                    'outer_fold': outer_fold,\n",
    "                    'inner_fold': inner_fold,\n",
    "                    'batch_size': batch_size,\n",
    "                    'hidden_channels': hidden_channels,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'val_auc': val_auc\n",
    "                }\n",
    "\n",
    "                results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_grid_search(models, pytorch_custom_dataset, results_dir, use_early_stopping):\n",
    "    all_results = []\n",
    "\n",
    "    for name, model_class in models.items():\n",
    "        print(f\"Running Grid Search for {name} model...\")\n",
    "\n",
    "        if \"GCN\" in name:\n",
    "            conv_type = GCNConv\n",
    "        elif \"GAT\" in name:\n",
    "            conv_type = GATConv\n",
    "        elif \"GraphSAGE\" in name:\n",
    "            conv_type = SAGEConv\n",
    "        else:\n",
    "            conv_type = None  # For GINModel, no conv_type is needed\n",
    "\n",
    "        results = grid_search(\n",
    "            model_class=model_class,\n",
    "            conv_type=conv_type,\n",
    "            pytorch_custom_dataset=pytorch_custom_dataset,\n",
    "            results_dir=results_dir,\n",
    "            model_name=name,\n",
    "            use_early_stopping=use_early_stopping\n",
    "        )\n",
    "\n",
    "        all_results.extend(results)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def convert_labels_to_class_indices(pytorch_custom_dataset):\n",
    "    label_mapping = {\n",
    "    (1, 0): 0,\n",
    "    (0, 1): 1,\n",
    "    (1, 1): 2\n",
    "}\n",
    "    for data in pytorch_custom_dataset:\n",
    "        label_tuple = tuple(data.y.numpy())\n",
    "        data.y = torch.tensor([label_mapping[label_tuple]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e1ecc-3642-4b9e-ae6c-756b7682ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated from DILIGeNN Data Preprocessing.ipynb\n",
    "df_clintox_filtered_cleaned = pd.read_csv('chem_data/clintox2_standardised_cleaned.csv', index_col = 0)\n",
    "\n",
    "clintox_min = Graph_basic(dataframe = df_clintox_filtered_cleaned,\n",
    "                              smiles_col = 'smiles',\n",
    "                             label_col = 'label',\n",
    "                             root = 'custom_data/minimal_feature/clintox2_min'\n",
    "                            ) \n",
    "convert_labels_to_class_indices(clintox_min)\n",
    "\n",
    "clintox_cus_2_std = Graph_custom(dataframe = df_clintox_filtered_cleaned,\n",
    "                              smiles_col = 'smiles_std',\n",
    "                             label_col = 'label',\n",
    "                             root = 'custom_data/clintox2/cus2_std/'\n",
    "                            )\n",
    "convert_labels_to_class_indices(clintox_cus_2_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227588fb-1549-421e-a42f-1345dbb15eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'GCN_Optimised': GNNModel,\n",
    "    'GAT_Optimised': GNNModel,\n",
    "    'GraphSAGE_Optimised': GNNModel,\n",
    "    'GIN_Optimised': GINModel\n",
    "}\n",
    "\n",
    "pytorch_custom_dataset = clintox_cus_2_std\n",
    "results_dir = './results/grid_search/clintox2'\n",
    "\n",
    "all_results = run_grid_search(\n",
    "    models,\n",
    "    pytorch_custom_dataset,\n",
    "    results_dir,\n",
    "    use_early_stopping = True\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(os.path.join(results_dir, 'grid_search_results.csv'), index=False)\n",
    "\n",
    "with open(os.path.join(results_dir, 'grid_search_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ea14c-5b9f-4cf7-9f20-fd7716a0742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'GCN_Optimised': GNNModel,\n",
    "    'GAT_Optimised': GNNModel,\n",
    "    'GraphSAGE_Optimised': GNNModel,\n",
    "    'GIN_Optimised': GINModel\n",
    "}\n",
    "\n",
    "pytorch_custom_dataset = clintox_min\n",
    "results_dir = './results/grid_search/clintox2_min'\n",
    "\n",
    "all_results = run_grid_search(\n",
    "    models,\n",
    "    pytorch_custom_dataset,\n",
    "    results_dir,\n",
    "    use_early_stopping = True\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(os.path.join(results_dir, 'grid_search_results.csv'), index=False)\n",
    "\n",
    "with open(os.path.join(results_dir, 'grid_search_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
