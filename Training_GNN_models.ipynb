{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6995aa97-6374-4100-bd64-ca260dab2d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from rdkit import RDLogger\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw, rdMolTransforms\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "IPythonConsole.drawOptions.addAtomIndices = True\n",
    "IPythonConsole.molSize = 300,300\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import SAGEConv, GCNConv, GATConv, GINConv\n",
    "from torch_geometric.data import Data, Dataset, InMemoryDataset#, DataLoader depreciated, use below\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cfe76b9-3519-40b6-9096-52277dd82b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self, conv_type, hidden_channels, num_features, batch_norm=False, weight_init=True, dropout_rate=0.0):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = conv_type(num_features, hidden_channels*2)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels*2) if batch_norm else None #BatchNorm1D -> GraphNorm\n",
    "        \n",
    "        self.conv2 = conv_type(hidden_channels*2, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels) if batch_norm else None\n",
    "        \n",
    "        self.conv3 = conv_type(hidden_channels, hidden_channels//2)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "        self.lin = torch.nn.Linear(hidden_channels//2, 1)\n",
    "        if weight_init:\n",
    "            self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Apply Xavier initialisation to the weights of a given module.\"\"\"\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, torch_geometric.nn.MessagePassing):\n",
    "            # Initialising weights of the MessagePassing (convolution) layers\n",
    "            for param in module.parameters():\n",
    "                if param.dim() > 1:  # Only initialise weights, not biases\n",
    "                    torch.nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        if self.bn1:\n",
    "            x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        #if self.dropout:\n",
    "        #    x = self.dropout(x)\n",
    "            \n",
    "        x = self.conv2(x, edge_index)\n",
    "        if self.bn2:\n",
    "            x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        #if self.dropout:\n",
    "        #    x = self.dropout(x)\n",
    "            \n",
    "        x = self.conv3(x, edge_index)\n",
    "        #if self.bn3:\n",
    "        #    x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = torch_geometric.nn.global_mean_pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "class GINModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_features, batch_norm=False, weight_init=True, dropout_rate=0.0):\n",
    "        super(GINModel, self).__init__()\n",
    "        # Define MLP for GINConv\n",
    "        def mlp(input_dim, output_dim):\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_dim, output_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(output_dim, output_dim)\n",
    "            )\n",
    "\n",
    "        self.conv1 = torch_geometric.nn.GINConv(mlp(num_features, hidden_channels*2))\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels*2) if batch_norm else None\n",
    "        \n",
    "        self.conv2 = torch_geometric.nn.GINConv(mlp(hidden_channels*2, hidden_channels))\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels) if batch_norm else None\n",
    "        \n",
    "        self.conv3 = torch_geometric.nn.GINConv(mlp(hidden_channels, hidden_channels//2))\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "        self.lin = torch.nn.Linear(hidden_channels//2, 1)\n",
    "\n",
    "        # Apply Xavier initialisation\n",
    "        if weight_init:\n",
    "            self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Apply Xavier initialisation to the weights of a given module.\"\"\"\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, GINConv):\n",
    "            # Initialising weights of the GINConv's MLPs\n",
    "            for layer in module.nn:\n",
    "                if isinstance(layer, torch.nn.Linear):\n",
    "                    torch.nn.init.xavier_uniform_(layer.weight)\n",
    "                    if layer.bias is not None:\n",
    "                        torch.nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        if self.bn1:\n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(x) # Only if batch normalisation is performed\n",
    "        #if self.dropout:\n",
    "        #    x = self.dropout(x)\n",
    "            \n",
    "        x = self.conv2(x, edge_index)\n",
    "        if self.bn2:\n",
    "            x = self.bn2(x)\n",
    "            x = F.relu(x)\n",
    "        #if self.dropout:\n",
    "        #    x = self.dropout(x)\n",
    "            \n",
    "        x = self.conv3(x, edge_index)\n",
    "        #if self.bn3:\n",
    "        #    x = self.bn3(x)\n",
    "        #    x = F.relu(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = torch_geometric.nn.global_mean_pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed6593b-46c9-437a-b5ce-1c5953528b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimiser, criterion, device, threshold = 0.5): # added\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y.view(-1, 1).float())\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = (out > threshold).float() # added\n",
    "        correct += pred.eq(data.y.view(-1, 1)).sum().item()\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion, device, threshold = 0.5): # added\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y.view(-1, 1).float())\n",
    "            total_loss += loss.item()\n",
    "            pred = (out > threshold).float() # added\n",
    "            correct += pred.eq(data.y.view(-1, 1)).sum().item()\n",
    "            preds.append(out)\n",
    "            labels.append(data.y.view(-1, 1))\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    return total_loss / len(loader), correct / len(loader.dataset), roc_auc_score(labels.cpu(), preds.cpu()), f1_score(labels.cpu(), (preds > threshold).float().cpu()), matthews_corrcoef(labels.cpu(), (preds > threshold).float().cpu()) # added # modified for MCC\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_score, model):\n",
    "        score = -val_score\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_score, model)\n",
    "        elif score < self.best_score - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_score, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_score, model):\n",
    "        self.best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c476f11f-9c8d-4c89-b63f-ced7e4528d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_and_models(pytorch_custom_dataset, batch_norm, weight_init, batch_size=32, hidden_channels=64, dropout_rate=0.3, seed=42):\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # split into indices 10 fold cross-validation\n",
    "    num_samples = len(pytorch_custom_dataset)\n",
    "    indices = np.arange(num_samples)\n",
    "    skf_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    targets = [data.y.item() for data in pytorch_custom_dataset]\n",
    "\n",
    "    data_loaders = []\n",
    "\n",
    "    for train_indices, test_indices in skf_outer.split(indices, targets):\n",
    "        train_dataset = Subset(pytorch_custom_dataset, train_indices)\n",
    "        test_dataset = Subset(pytorch_custom_dataset, test_indices)\n",
    "\n",
    "        # Further split train_indices into train and validation sets \n",
    "        skf_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)  \n",
    "        inner_train_indices, val_indices = next(skf_inner.split(train_indices, np.array(targets)[train_indices]))  \n",
    "\n",
    "        inner_train_dataset = Subset(pytorch_custom_dataset, inner_train_indices)  \n",
    "        val_dataset = Subset(pytorch_custom_dataset, val_indices)  \n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(inner_train_dataset, batch_size=batch_size, shuffle=False)  \n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  \n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        data_loaders.append((train_loader, val_loader, test_loader))  \n",
    "\n",
    "    # Define model parameters\n",
    "    num_features = pytorch_custom_dataset.num_features\n",
    "\n",
    "    # Define models dictionary\n",
    "    models = {\n",
    "        'GCN_Optimised': GNNModel(GCNConv, hidden_channels, num_features, batch_norm, weight_init, dropout_rate=dropout_rate),\n",
    "        'GAT_Optimised': GNNModel(GATConv, hidden_channels, num_features, batch_norm, weight_init, dropout_rate=dropout_rate),\n",
    "        'GraphSAGE_Optimised': GNNModel(SAGEConv, hidden_channels, num_features, batch_norm, weight_init, dropout_rate=dropout_rate),\n",
    "        'GIN_Optimised': GINModel(hidden_channels, num_features, batch_norm, weight_init, dropout_rate=dropout_rate)\n",
    "    }\n",
    "\n",
    "    return data_loaders, models\n",
    "\n",
    "def run_model(models, data_loaders, num_runs=5, num_epochs=30, learning_rate=0.0001, \n",
    "              use_early_stopping=False, patience=10, delta=0.001):\n",
    "    \n",
    "    results = {name: {'accuracy': [], 'f1': [], 'auc': [], 'mcc': [], 'train_loss': [], 'val_loss': []} \n",
    "               for name in models.keys()}\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    threshold = 0.5  # For binary classification\n",
    "    \n",
    "    for name, model_class in models.items():\n",
    "        print(f\"\\nRunning {name} model...\")\n",
    "        for run in range(num_runs):\n",
    "            print(f\"Run {run + 1}/{num_runs}\")\n",
    "            \n",
    "            for fold, (train_loader, val_loader, test_loader) in enumerate(data_loaders):  \n",
    "                print(f\"Fold {fold + 1}/{len(data_loaders)}\")\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Set random seed\n",
    "                seed = run\n",
    "                torch.manual_seed(seed)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.manual_seed(seed)\n",
    "                \n",
    "                model = model_class.to(device)\n",
    "                for layer in model.children(): ############ Use this all the time!!\n",
    "                    if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "                        torch.nn.init.xavier_uniform_(layer.weight)\n",
    "                    if hasattr(layer, 'bias') and layer.bias is not None:\n",
    "                        torch.nn.init.zeros_(layer.bias)\n",
    "                optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                criterion = torch.nn.BCEWithLogitsLoss()\n",
    "                \n",
    "                best_val_auc = 0\n",
    "                early_stopping = EarlyStopping(patience=patience, delta=delta)\n",
    "                for epoch in range(1, num_epochs + 1):\n",
    "                    train_loss, train_acc = train(model, train_loader, optimiser, criterion, device, threshold=threshold)\n",
    "                    val_loss, val_acc, val_auc, _, _ = evaluate(model, val_loader, criterion, device, threshold=threshold)  \n",
    "                    \n",
    "                    results[name]['train_loss'].append(train_loss)\n",
    "                    results[name]['val_loss'].append(val_loss)\n",
    "                    \n",
    "                    if use_early_stopping:\n",
    "                        early_stopping(val_auc, model)\n",
    "                        if early_stopping.early_stop:\n",
    "                            print(f\"Early stopping at epoch {epoch}\")\n",
    "                            break\n",
    "                \n",
    "                best_model = early_stopping.best_model if use_early_stopping else model\n",
    "                \n",
    "                test_loss, test_acc, test_auc, test_f1, test_mcc = evaluate(best_model, test_loader, criterion, device, threshold=threshold)  \n",
    "                \n",
    "                results[name]['accuracy'].append(test_acc)\n",
    "                results[name]['f1'].append(test_f1)\n",
    "                results[name]['auc'].append(test_auc)\n",
    "                results[name]['mcc'].append(test_mcc)\n",
    "                \n",
    "                print(f'{name} Fold {fold + 1} Run {run + 1}\\n'\n",
    "                      f'Test Acc: {test_acc:.4f}, Test F1: {test_f1:.4f}, Test AUC: {test_auc:.4f}, Test MCC: {test_mcc:.4f}\\n'\n",
    "                      f'Val Acc: {val_acc:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}\\n'\n",
    "                      f'Train Acc: {train_acc:.4f}, Train Loss: {train_loss:.4f}')\n",
    "                end_time = time.time()\n",
    "                total_time = (end_time - start_time) / 60\n",
    "                print(f\"Fold total time taken: {total_time:.2f} minutes\")\n",
    "            \n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
